---
title: how to write fast code - Module 2
date: 2017-03-16 20:00:08
categories: STUDY
tags: how to write fast code
---

### M2-1

#### What are the exploitable levels of parallelism in a multicore processor?

<!--more-->
- SIMD-Level Parallelism: Exploited using vectorizing compiler and hand-core intrinsics
- SMT-Level Parallelism: OS abstract it to core-level parallelism
- Core-level Parallelism: Using threads to describe work sone on different cores
#### What is SPMD? And how to use OpenMP to do SPMD?

SPMD: Single Program Multiple Data (pragma omp for)

- Programmer must explicitly specify what each thread must do differently
- The division of work is hard-core in the program
- OpenMP: an API for shared memory multiprocessor.

#### What’s the difference between “critical” and “atomic”?

- OpenMP critical: anything in a critical region, the OpenMP infrastructure will make sure it's sequential. The enclosed block will be executed only one thread at a time and that's simultaneously executed by multiple threads.
- Atomic: only the memory update portion of the statement, the instruction will be performing atomically

```
	#pragma opm critical
	{sum += 4.0/(1.0+x*x);}
	
	tmp = 4.0/(1.0+x*x);
	#pragma omp atomic
	{sum += tmp;}
```
#### How to reduce synchronization cost and avoid “false sharing”?

false sharing: if two threads are writing to the same cache line, conflicts occurs. Even if the adddress differs, one will still suffer performance penalty

Solution for False Sharing:(local_sum)

- Be aware of the cache line sizes for a platform
- Avoid accessing the same cahce line from different threads

```
	//race condition:
	for(i=id,sum=0;i<num_steps;i+=nthreads){
		sum+=i;
	}
	//false sharing:
	double sum[NUM_THREADS];
	for(i=id,sum[id]=0;i<num_steps;i+=nthreads){
		sum[id]+=i;
	}
	// eliminate false sharing:
	double local_sum=0;
	for(i=id;i<num_steps;i+=nthreads){
		local_sum+=i;
	}

```
#### What are the scheduling, reduction, data sharing, and synchronization options for OpenMP?

- scheduling: The chedule clause affects how loop interations are mapped onto threads
	- schedule(static[,chunk])
	- schedule(dynamic[,chunk])
	- schedule(guided[,chunk])
	- schedule(runtime)

- reduction(op: list)
	- A local copy of each list variable is made and initialized depending on the "op"(eg. 0 for "+")
	- Updates occur on the local copy
	- local copies are reduced into a single value and combined with tge original global value

- data sharing
	- shared
	- private
	- firstprivate
	- lastprivate

- synchronization
	- ordered: #pragma omp for order reduction(+:res); #pragma ordered
	- barrier: #pragma omp barrier; #pragma omp nowait
	- single: #pragma omp single
#### How is this relevant to writing fast code?

![module2-1](https://raw.githubusercontent.com/Annashuo/hello-world/master/M2-1.png)

### M2-2
#### Why does naïve matrix-multiply does not achieve peak performance on the CPU?

Naive matrix-multiply is really no improvement over matrix vector multiply because for matrix vector multiply that's exacly what our inner loop is we just repeat it back n times. The naive matrix-multiply does not achieve peak performance on the CPU is because of cache hierarchy. Page misses every iteration for large matrices.

#### What are the different data layouts for matrices?

- Row major 
- Column major

#### Is blocking sufficient?

- Block size adaptation for appropriate caches
- Register-level blocking
- Copy optimization(data layout)
- Optimizing the mini-matrix-multiply(base case)
- Multi-level blocking
- Multi-level copying

#### What can be learned from this for other computations?

- Strength reduction
- Function inlining
- Loop unrolling
- Common subexpression elimination
- Load/store elimination
- Table lookups
- Branch elimination

### M2-3

#### What is the roofline model? What are the metrics and axis used?

**Roofline Model**: Synthesize communication, computation, and locality into a single visually-intuitive performance figure using bound and bottleneck analysis.

On the x axis, we have the number of floating point operations per second divided by the bytes that we need to load from the DRAM. So, it's a, a floating point divided by DRAM byte load ratio.

On the y axis, we have attainable giga flops per second. So, this is a peak amount of computation per second that one can do.

Attainable Performance=min{FLOP/s with Optimization, AI*Bandwidth with Optimizations}

**Arithmetic Intensity**: the number of floating point operations to run the program, divided by, the number of bytes assessed, accessed from the main memory.

**floating point operations per second**: floating point operations for a second with optimization, with the computation, through performance

- Computational Ceilings(in-core parallelism)
	- If the code is dominated by adds or multiplies, then attainable performance is half of peak
	- If instructions aren't SIMDized, attainable performance will be halved
	- If we don't express 4-way ILP, performance will drop by as much as 4x

- Communication Ceilings(bandwidth)
	- Explicit software prefetch instructions are required to achieve peak bandwidth
	- (NUMA) As such memory traffic must be correctly balanced among the two sockets to achieve good Stream bandwidth

- Locality Walls(Arithmetic Intensity = FLOPs/(Conflict+Capacity+Allocations+Compulsory))
	- compulsory miss traffic
	- write allocation behavior
	- capacity miss traffic
	- conflict miss traffic

![Roofline](https://raw.githubusercontent.com/Annashuo/hello-world/master/Roofline.png) 

![Optimization_Categorization](https://raw.githubusercontent.com/Annashuo/hello-world/master/Optimization_Categorization.png)
#### What’s the difference between:"flop's per memory instruction" from "flop's per DRAM byte"?

Memory Instructions: memory operations concurrently managed

True Arithmetic Intensity(AI)~Total Flops/Total DRAM Bytes

Where the memory instructions are an artifact of the code that you write. And the DRAM bytes is how that code is being mapped to a particular platform.

On the roofline model, what we see is the operating point initially, could be considered as at one particular flop per DRAM byte ratio. 

- An example where the memory instructions is greater than the DRAM bytes: With the consideration of properties like write, allocate, traffic. We may have a lower number of floating point operations per DRAM bytes used.

- An example where the DRAM bytes is greater than the memory instructions: Although we are executing a small number of memory instructions, they end up fetching a lot od data from memory because the data layout is not optimal for fetching data. And in order to fetch one memory instruction, we need to fetch a whole cache line from the DRAM to the cache hierarchy. 
#### Consider an image Image[height][width]. If one were to stride through the columns of values, what would be the effects? How would they be mapped to the roofline?

If one were to stride through the columns of values, it will cause compulsory misses and conflict misses, and will cause walls in roofline.
#### How does one model incomplete SIMDization (e.g. half the flop's can be SIMDized), insufficient ILP (some dependent flop's), or an imbalance between FPMUL's and FPADD's on the roofline ?

If instructions aren't SIMDized, attainable performance will be halved.

If we don't express 4-way ILP, performance will drop by as much as 4x

If the code is dominated by adds, then attainable performance is half of peak
#### How would one model {branch mispredicts, TLB misses, or too many streams for the prefetchers} on the roofline?Branch mispredicts, TLB misses or too many streams for the prefetchers will minimize the memory bandwidth, which limits the roofline.


![Roofline](https://raw.githubusercontent.com/Annashuo/hello-world/master/roofline_mindmap.png) 