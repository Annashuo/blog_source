---
title: how to write fast code - Module 3
date: 2017-03-17 20:00:08
categories: STUDY
tags: how to write fast code
---

### M3-1
#### What’s the Difference between Multicore and Manycore?

<!--more-->

- Multicore: Optimized for reducing execution latency for a few threads
	- Sophisticated instruction controls, large caches
	- Each core optimized for executing a single thread

- Manycore: Assumes 1000-way concurrency readily availabel in applications
	- More resources dedicated to compute
	- Cores optimized for aggregate throughput, deemphasizing individual performance
#### When does using a GPU make sense?

- Applications with a lot of concurrency
- Some memory intensive applications
- Advantage diminidhes when task granularity becomes too large to fit in shared memory

#### What is the memory hierarchy inversion? And why is it there?    

Memory hierarchy inversion: the higher level memory L1 or L2 cache are actually smaller than the register file.

Because there is shared memory.

#### What is the memory wall? How to get around it?

Memory wall: performance gap between a processor and the piece of memory, that is getting longer, and longer to fetch a piece of data from memory to process in the processor.

Manycore processors utilize application concurrency to hide memory latency.
#### Why warps?

Warps: Software abstraction to hide an extra level of architextural complexity

1 Streaming Mulprocessor(SM)= at most 8 blocks

1 block=??? warps

1 warp=32 threads. (32-wide SIMD)
#### How do we deal with GPUs of different sizes?

Well what CUDA provides is an abstraction, that, can scale across all these different sizes. A thread block is a bock of threads that can execute at the same time while sharing some context. ComputaIon is grouped into blocks of independent, concurrently executable work
#### What are the implications of the thread block abstraction?

- Threads are the computation performed in each SIMD lane in a core
	- CUDA provides a SIMT programming abstraction to assist users

- SIMT: Single Instruction Multiple Threads
	- A single instruction controls multiple processing element
	- Different from SIMD – SIMD exposes the SIMD width to the programmer
	- SIMT abstract the #threads in a thread block as a user - specified parameter

- SIMT enables programmers to write thread-level parallel code for    
	- Independent,scalarthreads
	- Data-parallel code for coordinated threads
- For functional correctness, programmers can ignore SIMT behavior
- For performance, programmers can tune applications with SIMT in mind
#### How do threads communicate with each other?

Using shared memory and L1 cache.

![module3](https://raw.githubusercontent.com/Annashuo/hello-world/master/module3.png)
#### What is the caveat in synchronizing threads in a thread block?

__syncthreads()   
- waits until all threads in the thread block have reached this point and all global and shared memory accesses made by these threads prior to __syncthreads() are visible to all threads in the block
- used to coordinate communication between the threads of the same block

#### CUDA Function Types

- \__global__   - Executed on the device   - Callable from the host only   - \__global__ functions must have void return type   - Any call to a \__global__ function must specify its execution configuration <<< >>>   - A call to a \__global__ function is asynchronous- \__device__   - Executed on the device   - Callable from the device only- \__host__   - Executed on the host   - Callable from the host only

### M3-2
#### What are the three ways to improve execution throughput?

- Maximizing Memory Throughput
	- SoA vs AoS
	- Memory coalescing
	- Use of shared memory
	- Memory bank conflict
	- Padding

- Maximizing Instruction Throughput
- Maximizing Scheduling Throughput
#### When to use SOA vs AOS?

Example one: You have a list of coordinates, and you want to find the distance from the origin to all points

- Struct of Array(**better**): index all these x, y, and z's with the thread ID. for each thread we're actually picking out one of these index. And we would have multiple of these threads executing at the same time.
- Array of Struct(**worse**): each thread is going to be loading operands from memory locations with a certain stride. In order to satisfy the operand for one operation it actually needs to go to memory and load a bunch of things that are not gonna be used for a particular calculation. And without any sort of shared memory buffering.

Example two: Find the distance from the origin to a list of points estimated to be ~1% of all points

- Struct of Array(**worse**): we are gonna have a huge strides between all these consecutive elements.
- Array of Struct(**better**): we're gonna be loading chunks of these tuples at a time so that we can be more efficient in loading blocks of memory at a time. We're better utilizing the memory bandwidth by loading these consecutive elements for each array at a time
#### What is memory Coalescing? When to use it? Why is it important?  

Memory Coalescing: Combine multiple memory accesses generated from multiple threads into a single physical transaction.

It happens when memory access.

It can increase DRAM memory bandwidth and throughput.

Some rules for maximizing DRAM memory bandwidth: 

- Possible bus transaction sizes: 32B, 64B,128B
- Memory segment must be aligned: First address = multiple of segment size
- Hardware coalescing for each half-warps: 16-word wide 
  
#### What is shared memory? How to use it?

Registers and shared memory are on-chip memories. Variables that reside in these types of memory can be accessed at very high speed in a highly parallel manner.

Registers are allocated to individual threads; each thread can only access its own registers.

Shared memory is allocated to thread blocks; all threads in a block can access variables in the shared memory locations allocated to the block. Shared memory is an efficient means for threads to cooperate by sharing their input data and the intermediate results of their work.

How to use shared memory?

- Declared a fixed sized variable at compile time
- Define a size to be used at run time
#### What is memory bank conflict? How to work around it?

Shared memory has 32 banks. Each bank has a bandwidth of 32 bits per two clock cycles(2 cycle latency)

A bank conflict occurs if two or more threads access any bytes within **different 32-bit words belonging to the same bank**.

Using padding technique to offset memory bank conflicts.
#### What is branch divergence?

Branch divergence occurs only within a warp. Full efficiency is realized when all 32 threads of a warp agree on their path. To optimize, factor out decision variables to have shorter sequence of divergent code.
#### How to optimize for instruction mix?

Compiler Assisted Loop Unrolling(In CUDA, #pragma unroll)
#### What is occupancy? How to model/measure it?

Occupancy: 

- Ability of a CUDA kernel to occupy concurrent contexts in a SM(Streaming Multiprocessor)
- Specifically, the ratio of active warps to the maximum number of warps supported
- Helpful in determining how efficient the kernel could be on the GPU
#### How to use the code profiler with CUDA?

CUDA Profiler Tutorial – by Erik Reed

### M3-3
#### What are the important properties of a Map function?

Map: a function that applies a given function to each element of a list and return a list of result.

Two properties:

- Side-effect free: Only returning a value, no modificaGons of state with the rest of the application
- Independent: Has an independent piece of work, where its input does not depend on another function

#### What are the important properties of a Reduce function?

Reduce: A function that takes in a list of objects and builds a return value

Three properties:

- Associativity
- Allows elements to be reduced in parallel in a "tree"
- In CUDA, the synchronization has to be managed by the programmer

```
	//totally log2(blockDim.x) iterations, all have branch divergence
	unsigned int i=threadIdx.x;
	for(unsigned int stride=1; stride<blockDim.x; stride*=2){
		_syncthreads();
		if(t%(2*stride)==0)
			sum[t]+=sum[t+stride];
	}
	// another version: only 5 iterations have branch divergence
	for(unsigned int stride=blockDim.x/2; stride>1; stride/=2){
		_syncthreads();
		if(t<stride)
			sum[t]+=sum[t+stride];
```

#### What are the important properties of a Scan function?	

Scan(Prefix Sum):
Takes a binary associative operator ⊕ with identity I, and an array of n elements [a0, a1, ..., an-1] and returns the ordered set [I, a0, (a0 ⊕ a1), ..., (a0 ⊕ a1 ⊕ ... ⊕ an-2)].

Trust library of CUDA

```
	#include <thrust/scan.h>           	int data[6] = {1, 0, 2, 2, 1, 3};  
	thrust::exclusive_scan(data, data + 6, data); // in-place scan   	// data is now {0, 1, 1, 3, 5, 6} 
```

#### How to compact an array in a data-parallel way?	

Compaction: Removing elements from an array - take in an array, and produce an shorter array

- Map–create flags(“1”keep,“0”remove)    
- Scan–compute index
- Map–copy to new array

#### How to find unique elements in an array in a data-parallel way?

FindUniq: Removing duplicates from an array – take in an set, produces a equal or smaller set of unique values

- Sort
- Map – flag when ith and (i-1)th element differ [0] = 1
- Scan–create compaction index
- Map–copy to new array

### M3-4
#### What are parallel software patterns?

A parallel software pattern is a generalizable solution to a class of recurring problems that occurs in the design of parallel software.
#### What are the three goals the software patterns aim to achieve?

- Define a set of vocabularies to communicate
- Present a set of expert techniques for beginners to learn
- Allows experts to more quickly design complex systems
#### What is a software architecture?

A software architecture is ahierarchical composition of:
- Computational patterns – the atoms- Structural patterns – the molecular bonds
#### How is it important for writing fast code?#### What are the five categories of patterns in OPL?

![OPL](https://raw.githubusercontent.com/Annashuo/hello-world/master/OPL.png)
#### What are the nine sections in an OPL pattern?